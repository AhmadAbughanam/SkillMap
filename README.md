# SkillMap-Flask ğŸš€

**SkillMap-Flask** is a self-improvement tracker that allows users to:

- Create and manage skills
- Track progress with milestones
- Automatically generate AI-powered insights using a local LLM

---

## ğŸ§  Features

- ğŸ¯ **Skill Management** â€“ Create, edit, delete skills by category and description
- ğŸ“ˆ **Milestone Tracking** â€“ Log progress updates with timestamps and notes
- ğŸ¤– **LLM Insights** â€“ Auto-generate learning insights using a locally hosted LLM
- ğŸ“Š **Insight Dashboard** â€“ View AI insights by section, skill, or generation time
- ğŸ§± Built with:
  - Flask
  - SQLAlchemy
  - Bootstrap
  - llama-cpp-python (Mistral 7B Instruct GGUF)

---

## ğŸ“‚ Folder Structure

```

SkillMap-Flask/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ templates/        # HTML templates
â”‚   â”œâ”€â”€ static/           # Static files (CSS, JS)
â”‚   â”œâ”€â”€ routes/           # Flask blueprints (skills, LLM)
â”‚   â”œâ”€â”€ utils/            # LLM logic (llm\_client.py)
â”‚   â”œâ”€â”€ models.py         # SQLAlchemy models
â”‚   â””â”€â”€ **init**.py       # App factory
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-instruct-v0.1.Q2\_K.gguf  # Local LLM model
â”‚
â”œâ”€â”€ venv/                 # Virtual environment
â”œâ”€â”€ test.py               # Manual insight generation test
â”œâ”€â”€ config.py             # (Optional) Config settings
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

---

## âš™ï¸ Setup Instructions

1. **Clone the repo**:
   ```bash
   git clone https://github.com/your-username/SkillMap-Flask.git
   cd SkillMap-Flask
   ```

````

2. **Create a virtual environment**:

   ```bash
   python -m venv venv
   venv\Scripts\activate
   ```

3. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Download and place the LLM model** (e.g., Mistral 7B Instruct GGUF):

   * Place it in `app/models/` and update the `model_path` in `llm_client.py`.

5. **Run the app**:

   ```bash
   flask run
   ```
   another way by:

    python run.py

---

## ğŸ§  Local LLM Integration

* Uses `llama-cpp-python` to load a local GGUF model.
* Insights are generated when:

  * A new skill is created
  * A milestone is added
* Insight content is stored in the `llm_insights` table and displayed in the dashboard.

---

## ğŸ› ï¸ Developer Notes

* Make sure `llama_cpp` is installed and your system supports AVX.
* Running on CPU; GPU support requires additional setup.
* Use `test.py` to verify model and generation output manually.

---

## âœ… Example Prompt

```python
from app.utils.llm_client import generate_llm_insight
from app.models import Skill

with app.app_context():
    skill = Skill.query.first()
    insights = generate_llm_insight(skill)
    print(insights)
```

---

## ğŸ“ƒ License

MIT License

````
